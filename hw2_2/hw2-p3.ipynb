{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 2 Problem 3 (15 points) [TA: Sogol Mansouri]** \n",
    "\n",
    "In this workshop, you'll looking at evaluation metrics and hyperparameter tuning.\n",
    "\n",
    "## Instructions:\n",
    "1. Use github.ncsu.edu to submit your work repository if you have not done so yet. It is your responsbility to ensure the TAs have access to your work before the deadline. Make sure the repo is private.\n",
    "2. Do not modify the code structure given. Answer the questions in the designated space.\n",
    "\n",
    "All the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Loading Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5edb17c9957ed27e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "from sklearn import datasets\n",
    "# Remember you have to run this cell block before continuing!\n",
    "\n",
    "# We will use this random seed throughout to make things more deterministic for testing\n",
    "random_seed = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2fae64f787a55e35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1 Complete Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-alsjdaskjd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Loading the Data\n",
    "\n",
    "In this problem you will learn to calculate accuracy, precision, recall and f1-score for a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_data = pd.read_csv('../data/hw2_p3_data.csv')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample a subset of the dataset (stored as \"X\" and \"Y\") in order to avoid long running time. Now practice the train/test split function to create a training and testing dataset with the **\"random_seed\"** we defined at very beginning and the belowed **\"test_data_fraction\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cc7c5aa8554728a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = raw_data.sample(frac = 0.008, random_state = random_seed)\n",
    "X = data[\"sentence\"]\n",
    "Y = data[\"sentiment\"] \n",
    "\n",
    "test_data_fraction = 0.2\n",
    "## TODO: Make the train/test split this time\n",
    "X_train = None\n",
    "X_test = None\n",
    "Y_train = None\n",
    "Y_test = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the X features contain just one attribute, a string value from the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y values are 1 (for posiive sentiment) and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e928a8e016e1814e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_equal(len(X_train),9753)\n",
    "np.testing.assert_equal(len(Y_test),2439)\n",
    "np.testing.assert_equal(type(X_test),pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-307e8bfcba3ffd01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 Classification Pipeline\n",
    "\n",
    "In this problem you will create a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which is a nice tool provided by sklearn, to apply a list of transforms or model traning sequencially, pipeline class allows sticking multiple processes into a single estimator, it can be used to automate a machine learning workflow that involves multiple steps. For example:\n",
    "\n",
    "1. We need to extract our TFIDF features from the twitter data.\n",
    "2. Then we need to build a classifierusing our new TFIDF features.\n",
    "\n",
    "The advantage of putting these steps together into a pipeline, is that we can apply them repeatedly, e.g. to the training data and the test data.\n",
    "\n",
    "Additionally, for hyperparameter tuning, we often use k-fold cross validation, where we have many different training/test datasets, making the pipeline even more useful.\n",
    "\n",
    "Here is a brief example of how to use pipeline function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Put the example in a function so we don't overwrite our variables\n",
    "def example():\n",
    "    X, y = make_classification(random_state=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    # Create a pipeline composed of a standard scaler, and an SVC classifier\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "\n",
    "    # Fit the pipeline to the training data (just like you would any other classifier)\n",
    "    # Both scaling and fitting will be performed\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Use the fitted pipeline (i.e. the fit scaler and trained classifier) to score the test data\n",
    "    # This will first scale X_test, then predict y-values for this data, and finally compute accuracy\n",
    "    accuracy = pipe.score(X_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-191fb0e6eabae475",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now it is your turn to practice the pipeline function, create a pipeline including **1) tfidf vectorizer 2) KNN model (with 5 neighbors)**. Store the pipeline object as **KNN_pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a650d0c64a1d983",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "KNN_pipeline = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9a68db6e2ce04989",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_equal(type(KNN_pipeline),sklearn.pipeline.Pipeline)\n",
    "np.testing.assert_equal(len(KNN_pipeline.named_steps),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0041e152e9275783",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's evaluate the knn pipeline, use the training set to train the pipeline and make prediction on the testing set. Compare the result with the true labels of the testing set and calculate the accuracy score. Store the accuracy in the variable **\"test_accuracy\"** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4a9286f6128fcfe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_accuracy = None\n",
    "\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6c0d38b77b5989e4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(test_accuracy,0.63, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe923f6e8300e7b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see from the aboved result, the test accuracy is not as good as we expected. Now let's play with some hyperparameter tuning to see whether we can achieve better results with the optimized parameters, choosing the best value for $k$.\n",
    "\n",
    "Let's take a look at the below example on hyper-parameter tuning of the **KNN_pipeline** we created previously, using the [GridSearchCV](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html) function. The goal is to find the best value for $k$ (number of neighbors) in the KNN classifier, and we'll test the following values: [1,3,5,7,10].\n",
    "\n",
    "GridSearchCV will try each of these values, and then determine the best one by performing k-fold crossvalidation _within the training dataset_. If a value of k does well on unseen validation data, it will probably do well with test data.\n",
    "\n",
    "**HINT**: Select the best hyperparameter value only based on the training data\n",
    "\n",
    "**Note**: This may take a moment, since the GridSearch performs CV (multiple train/tests splits) with every hyperparameter value, leading to lots of model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter we hope to tune.\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": [1,3,5,7,10]\n",
    "}\n",
    "# Run the hyperparameter tuning with the training dataset, this will take a while to run.\n",
    "KNN_tuned_pipeline = GridSearchCV(KNN_pipeline, param_grid)\n",
    "KNN_tuned_pipeline.fit(X_train,Y_train)\n",
    "\n",
    "# Print out the best parameter as well as the cross-validated score of the best_estimator.\n",
    "print(\"Best parameter: {}, CV score = {}:\".format(KNN_tuned_pipeline.best_params_,KNN_tuned_pipeline.best_score_))\n",
    "\n",
    "# Now let's make prediction on the testing data with the best found parameters and check whether we can achieve higher accuracy.\n",
    "print(\"The testing accuracy with the best parameter is: {}\".format(accuracy_score(KNN_tuned_pipeline.predict(X_test), Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a12a3bd50b8feef8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now it is your turn. Create the similar pipeline (with the TFIDF vectorizer, followed by a model) for the decision tree and adaboost classifiers. Then perform hyperparameter tuning to create a tuned version of each pipeline. Make sure to fit the pipelines with the **training data** you have -- selecting a hyperparameter using the test data gives an unfair advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57c4979d5ad3e1be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For [decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), make sure to use the **`random_seed`** we created at very beginning. For the hyperparameter tuning, use the `GridSearchCV` to select the best criterion from **{“gini”, “entropy”}**, and the best  max_depth from **[2,3,4,5]**. \n",
    "\n",
    "Make sure to:\n",
    "\n",
    "1. Create the pipeline\n",
    "2. Tune it with CV\n",
    "3. Fit it to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eef14b7d60e9a971",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT_tuned_pipeline = None\n",
    "\n",
    "DT_param_grid = {\n",
    "    \"dt__criterion\": [], #TODO: Update this with parameter values\n",
    "    \"dt__max_depth\": [] #TODO: Update this with parameter values\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the best hyperparameters for DecisionTree\n",
    "print(\"Best parameter of DT: {}, CV score = {}:\".format(DT_tuned_pipeline.best_params_,DT_tuned_pipeline.best_score_))\n",
    "print(\"The testing accuracy with the best parameter of DT is: {}\".format(accuracy_score(DT_tuned_pipeline.predict(X_test), Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-033489960e767421",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "np.testing.assert_almost_equal(accuracy_score(DT_tuned_pipeline.predict(X_test), Y_test),0.562, decimal=3)\n",
    "np.testing.assert_almost_equal(DT_tuned_pipeline.best_score_,0.577, decimal=3)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For [Adaboost classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), make sure to use the **`random_seed`** we created at very beginning. For the hyperparameter tuning, use the `GridSearchCV` to select the best `n_estimators` from **[2,3,4]**, and the best  learning_rate from **[0.1,0.01,0.001]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb1444c7c39132bd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "ADABOOST_tuned_pipeline = None\n",
    "\n",
    "ADABOOST_param_grid = {\n",
    "    \"adaboost__n_estimators\": [], #TODO: Update this with parameter values\n",
    "    \"adaboost__learning_rate\": [] #TODO: Update this with parameter values\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8eb5cc4b2f14c0c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the best hyperparameters for Adaboost\n",
    "print(\"Best parameter of ADABOOST: {}, CV score = {}:\".format(ADABOOST_tuned_pipeline.best_params_, ADABOOST_tuned_pipeline.best_score_))\n",
    "print(\"The testing accuracy with the best parameter of ADABOOST is: {}\".format(accuracy_score(ADABOOST_tuned_pipeline.predict(X_test), Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fe2cd2577f11ddff",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "np.testing.assert_almost_equal(accuracy_score(ADABOOST_tuned_pipeline.predict(X_test), Y_test),0.542, decimal=3)\n",
    "np.testing.assert_almost_equal(ADABOOST_tuned_pipeline.best_score_,0.554, decimal=3)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0f5a6e8bccbae8b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 Evaluation using `classification_report` \n",
    "\n",
    "Sklearn also has a built in function called [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) that will give a handy summary of all the popular classification metrics. You can use this for the later questions.\n",
    "\n",
    "Below, we give an example of how to use the  function to summarize a model's performance.\n",
    "\n",
    "Precision, Recall and F1 are reported for **each class separately**. For the \"False\" row, a False is treated as the positive class. For the \"True\" row, the \"True\" is treated as the positive class. This is helpful because Precision and Recall are both sensitive to which class is considered positive. **Support** is the number of instances of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print a classification report for the KNN pipeline we created\n",
    "print(classification_report(Y_test, KNN_tuned_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7d263728e087f5d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now complete the following functions based on the descriptions in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-922f67c614238814",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def predict_with_pipeline(pipeline):\n",
    "    \"\"\" \n",
    "    You will implement a pipeline to predict for test-cases that performs the following tasks:\n",
    "        1. Use the tuned pipeline to predict labels Y_predict for X_test.\n",
    "        3. return the predictions\n",
    "        \n",
    "    Your inputs and outputs are as shown below:\n",
    "    \n",
    "    Input:\n",
    "        \n",
    "        pipeline: A classification/tuned pipeline. \n",
    "              Some example classifiers are: KNN_tuned_pipeline, DT_tuned_pipeline, ADABOOST_tuned_pipeline\n",
    "        \n",
    "        \n",
    "    Output:\n",
    "        predictions: Return the prediction by the classification pipeline on X_test\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "def ClassificationReport(Y_test, predictions,output_dict=True):\n",
    "    \"\"\" \n",
    "    You will implement this function to outputs the predictions classification report for test-cases that performs the following tasks:\n",
    "        1. This function will take three parameters:  the Y_test, predictions on X_test using the pipeline, and output_dict for dictionary format report\n",
    "        2. You can use the sklearn's classification_report function to generate the report\n",
    "        \n",
    "        \n",
    "    Your inputs and outputs are as shown below:\n",
    "    \n",
    "    Input:\n",
    "        \n",
    "        Y_test: The labels from in the Y_test\n",
    "        predictions: predictions on X_test using the predict_with_pipeline function.\n",
    "        output_dict: To generate the report in dictionary format.\n",
    "        \n",
    "    return:\n",
    "        \n",
    "        classification report  \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "KNN_predictions = predict_with_pipeline(KNN_tuned_pipeline)\n",
    "\n",
    "# If output_dict is False, we get a human-readable \n",
    "print(ClassificationReport(Y_test, KNN_predictions,output_dict=False))\n",
    "\n",
    "# Otherwise we can get the report as an object, to get individual values from it\n",
    "ClassificationReport(Y_test, KNN_predictions,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-35d4f3c5911b2289",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Public tests\n",
    "KNN_predictions = predict_with_pipeline(KNN_tuned_pipeline)\n",
    "KNN_report = ClassificationReport(Y_test, KNN_predictions)\n",
    "DT_predictions = predict_with_pipeline(DT_tuned_pipeline)\n",
    "ADABOOST_predictions = predict_with_pipeline(ADABOOST_tuned_pipeline)\n",
    "DT_report = ClassificationReport(Y_test, DT_predictions)\n",
    "ADABOOST_report = ClassificationReport(Y_test, ADABOOST_predictions)\n",
    "\n",
    "np.testing.assert_almost_equal(KNN_report['1']['precision'],0.709480122324159, decimal=3)\n",
    "np.testing.assert_almost_equal(DT_report['0']['recall'],0.8938271604938272, decimal=3)\n",
    "np.testing.assert_almost_equal(ADABOOST_report['0']['f1-score'],0.6605839416058393, decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cdd265f59d583b80",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.testing.assert_almost_equal(KNN_report['0']['precision'], 0.6378600823045267, decimal=3)\n",
    "np.testing.assert_almost_equal(DT_report['1']['recall'],0.23447712418300654, decimal=3)\n",
    "np.testing.assert_almost_equal(ADABOOST_report['1']['f1-score'],0.29811320754716986, decimal=3)\n",
    "\n",
    "### END HIDDEN TESTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the reports for each classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KNN:')\n",
    "print(ClassificationReport(Y_test, KNN_predictions,output_dict=False))\n",
    "\n",
    "print('\\n')\n",
    "print('Decision Tree:')\n",
    "print(ClassificationReport(Y_test, DT_predictions,output_dict=False))\n",
    "\n",
    "print('\\n')\n",
    "print('Adaboost:')\n",
    "print(ClassificationReport(Y_test, ADABOOST_predictions,output_dict=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e51c30a130bedf25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We would be most interested in the '1' category as this indicates the positive labels, as well as the accuracy scores. Through this process, you should already got three reports for each of the classifiers. Let's make some comparisons, print the reports for each of the classifiers, and which classifier has a better performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a49b9dcf85ef5d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 Evaluation using ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-611f2cf243b3746d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sklearn has some built in methods for [plotting ROC curves](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html).\n",
    "\n",
    "The dataset we'll be using for this exercise is the breast cancer dataset, which is used to tell if a certain individal might have breast cancer or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-488b13155465b85e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Plotting ROC Curves \n",
    "\n",
    "In this section, you will use sklearn API to compute ROC curves and corresponding AUC value. Specifically, you can use [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) and [roc_auc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) to compute these values.\n",
    "\n",
    "**Hint** You may also want to take a look at the `predict_proba` function from different models such as [decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba) and [Ada boost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). You will need to reliy on part of its output since ROC is computed based on proabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "roc_auc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def roc_auc(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    In this function, you will need to implement the following steps.\n",
    "        1. Use model to compute its probability of predicting a sample as positive for each sample in x_test.\n",
    "        2. Use the computed probability and y_test to compute ROC curve and its AUC value.\n",
    "        \n",
    "    Your inputs and outputs are as shown below:\n",
    "    \n",
    "    Input:\n",
    "        model: A sklearn classifier instance in our case a fine tuned classifier. Assuming it has predict_proba() function.\n",
    "        x_test: A numpy array of shape (n_test_rows, n_attributes) where n_test_rows refers to the number \n",
    "              of rows in your target dataset and n_attributes refers to the number of attributes.\n",
    "        y_test: A numpy array of shape (n_test_rows, ) where n_test_rows refers to the number \n",
    "              of rows in your target dataset and n_attributes refers to the number of attributes.\n",
    "        \n",
    "    Output:\n",
    "        fpr: A list of increasing false positive rates as a part of ROC curve.\n",
    "        tpr: A list of increasing true positive rates as a part of ROC curve.\n",
    "        thresholds: A list of decreasing thresholds as a part of ROC curve.\n",
    "        auc: A single float value that is the computed AUC value.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the ROC curves\n",
    "tree_fpr, tree_tpr, tree_thresh, tree_auc = roc_auc(DT_tuned_pipeline, X_test, Y_test)\n",
    "knn_fpr, knn_tpr, knn_thresh, knn_auc = roc_auc(KNN_tuned_pipeline, X_test, Y_test)\n",
    "ada_fpr, ada_tpr, ada_thresh, ada_auc = roc_auc(ADABOOST_tuned_pipeline, X_test, Y_test)\n",
    "\n",
    "plt.figure(0).clf()\n",
    "plt.plot(knn_fpr,knn_tpr,label=\"KNN, auc=\"+str(knn_auc))\n",
    "plt.plot(tree_fpr,tree_tpr,label=\"Decision Tree, auc=\"+str(tree_auc))\n",
    "plt.plot(ada_fpr,ada_tpr,label=\"Adaboost, auc=\"+str(ada_auc))\n",
    "\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c6ed5f7c66aef81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Intepreting ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f7497f03bb03d0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Take a look at the above ROC curves. How are they similar? How do they differ? Is one strictly better than the other? In what situations is one better than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
